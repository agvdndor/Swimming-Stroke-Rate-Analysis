{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitvenvvirtualenvbea9a94442cd49a38f8a0c2c03c4e380",
   "display_name": "Python 3.6.9 64-bit ('venv': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path as osp\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "# torch imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# root path of project\n",
    "from os import path as osp\n",
    "import sys\n",
    "\n",
    "# get root directory\n",
    "import re\n",
    "reg = '^.*/AquaPose'\n",
    "project_root = re.findall(reg, osp.dirname(osp.abspath(sys.argv[0])))[0]\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from lib.dataset.PoseDataset import PoseDataset\n",
    "\n",
    "from lib.models.keypoint_rcnn import get_resnet50_pretrained_model\n",
    "\n",
    "# utils\n",
    "from lib.utils.slack_notifications import slack_message\n",
    "from lib.utils.select_gpu import select_best_gpu\n",
    "from lib.utils.rmsd import kabsch_rmsd, kabsch_rotate, kabsch_weighted_rmsd, centroid, centroid_weighted, rmsd, rmsd_weighted\n",
    "\n",
    "# references import\n",
    "# source: https://github.com/pytorch/vision/tree/master/references/detection\n",
    "from references.engine import train_one_epoch, evaluate\n",
    "from references.utils import collate_fn\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_numpy_image(img_tensor):\n",
    "    return img_tensor.permute(1,2,0).detach().numpy()\n",
    "\n",
    "def get_max_prediction(prediction):\n",
    "    keypoints_scores = prediction[0]['keypoints_scores']\n",
    "    boxes = prediction[0]['boxes']\n",
    "    labels = prediction[0]['labels']\n",
    "    scores = prediction[0]['scores']\n",
    "    keypoints = prediction[0]['keypoints']\n",
    "\n",
    "    max_score = 0\n",
    "    max_box = []\n",
    "    for idx, box in enumerate(boxes):\n",
    "        if scores[idx].item() > max_score:\n",
    "            print(labels[idx].data.numpy())\n",
    "            max_score = scores[idx].item()\n",
    "            max_box = box\n",
    "            max_keypoints = keypoints[idx] \n",
    "            max_keypoints_scores = keypoints_scores[idx]\n",
    "    \n",
    "    return max_box.detach().numpy(), max_keypoints.detach().numpy(), max_keypoints_scores.detach().numpy()\n",
    "\n",
    "\n",
    "def plot_image_with_kps(img_tensor, kps_list, color_list= ['b', 'r', 'g']):\n",
    "    # plot positive prediction\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(tensor_to_numpy_image(img_tensor))\n",
    "    for kps, clr in zip(kps_list, color_list):\n",
    "        ax.scatter(np.array(kps)[:,0],np.array(kps)[:,1], s=10, marker='.', c=clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset to get a set of poses to match to\n",
    "ref_dataset = PoseDataset([osp.join(project_root,'data/vzf/freestyle/freestyle_1'), osp.join(project_root,'data/vzf/freestyle/freestyle_3'), osp.join(project_root,'data/vzf/freestyle/freestyle_4'), osp.join(project_root,'data/vzf/freestyle/freestyle_5'), osp.join(project_root,'data/vzf/freestyle/freestyle_6')], train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get model and select weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dir = osp.join(project_root, 'weights')\n",
    "weight_files = glob(osp.join(weight_dir,'*'))\n",
    "model = get_resnet50_pretrained_model()\n",
    "#print(weight_files)\n",
    "model.load_state_dict(torch.load(weight_files[-1], map_location=torch.device('cpu')))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show prediction + Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset \n",
    "test_dataset = PoseDataset([osp.join(project_root,'data/vzf/freestyle/freestyle_5')], train=False)\n",
    "\n",
    "# get prediction\n",
    "test_id = 3\n",
    "test_img, test_target = test_dataset[test_id]\n",
    "model.eval()\n",
    "prediction = model([test_img])\n",
    "\n",
    "# get poses pred and GT\n",
    "test_pred_box, test_pred_kp, test_pred_scores = get_max_prediction(prediction)\n",
    "test_gt_kp = test_target['keypoints'][0].detach().numpy()\n",
    "# set all visible\n",
    "test_gt_kp_all_vis = [[kp[0], kp[1], 1] for kp in test_gt_kp]\n",
    "\n",
    "# plot groundtruth\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(tensor_to_numpy_image(test_img))\n",
    "ax.scatter(np.array(test_gt_kp)[:,0],np.array(test_gt_kp)[:,1], s=10, marker='.', c='b')\n",
    "\n",
    "# plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(tensor_to_numpy_image(test_img))\n",
    "ax.scatter(np.array(test_pred_kp)[:,0],np.array(test_pred_kp)[:,1], s=10, marker='.', c='r')\n",
    "\n",
    "# plot positive prediction\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(tensor_to_numpy_image(test_img))\n",
    "#print(test_pred_scores)\n",
    "filter_inds = np.argwhere(test_pred_scores > 0).flatten()\n",
    "test_pred_kp_ftrd = test_pred_kp[filter_inds]\n",
    "ax.scatter(np.array(test_pred_kp_ftrd)[:,0],np.array(test_pred_kp_ftrd)[:,1], s=10, marker='.', c='r')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occluded=True will only use occluded gt points\n",
    "# side = right/left will only use those keypoints\n",
    "def filter_kps(pred_kps, ref_kps, scores, min_score=0, occluded=True, side = None, filter_lr_confusion=False, return_ind=False):\n",
    "    # merge all head keypoints into 'head'\n",
    "    pred_kps = pred_kps[4:]\n",
    "    ref_kps = ref_kps[4:]\n",
    "    scores = scores[4:]\n",
    "\n",
    "    filter_ind = np.argwhere(scores > min_score).flatten()\n",
    "\n",
    "    # Reduce left right confusion by filtering out far elbows and wrists that are estimated too close\n",
    "    # to their left counterpart\n",
    "    if filter_lr_confusion:\n",
    "        # get orientation of swimmer\n",
    "        # upper body keypoints: head, left_shoulder, right shoulder\n",
    "        upper_ind = [0, 1, 2]\n",
    "        # lower body keyponts: left hip, right hip, left knee, right knee\n",
    "        lower_ind = [7, 8, 9, 10]\n",
    "\n",
    "        upper_ind_vis = np.intersect1d(upper_ind, filter_ind)\n",
    "        lower_ind_vis = np.intersect1d(lower_ind, filter_ind)\n",
    "\n",
    "        # get mean x-co for upper and lower body\n",
    "        upper_x = mean([kp[0] for kp in pred_kps[upper_ind_vis]])\n",
    "        lower_x = mean([kp[0] for kp in pred_kps[lower_ind_vis]])\n",
    "\n",
    "        if upper_x < lower_x:\n",
    "            orientation = 'left'\n",
    "        else:\n",
    "            orientation = 'right'\n",
    "        \n",
    "        # [[left_elbow, right_elbow], [left_wrist, right_wrist]]\n",
    "        for joints in [[3,4], [5,6]]:\n",
    "            # if one of the joints is not present in filter ind\n",
    "            # do nothing\n",
    "            if joints[0] not in filter_ind or joints[1] not in filter_ind:\n",
    "                continue\n",
    "            left_joint = pred_kps[joints[0]]\n",
    "            right_joint = pred_kps[joints[1]]\n",
    "\n",
    "            if rmsd_weighted(left_joint, right_joint, weights=[1]) < 5:\n",
    "                print('joints filtered: {}',format(orientation))\n",
    "                if orientation == 'left':\n",
    "                    # filter out right joint\n",
    "                    filter_ind = filter_ind[filter_ind != joints[1]]\n",
    "                else:\n",
    "                    filter_ind = filter_ind[filter_ind != joints[0]] \n",
    "\n",
    "    if not occluded:\n",
    "        not_occluded = np.argwhere(ref_kps[:,2] > 0).flatten()\n",
    "        filter_ind = np.intersect1d(filter_ind, not_occluded)\n",
    "\n",
    "    if side == 'left':\n",
    "        left_ind = [0, 1 ,3 ,5 ,7 ,9, 11, 13, 15]\n",
    "        filter_ind = np.intersect1d(filter_ind, left_ind)\n",
    "    elif side == 'right':\n",
    "        right_ind = [0, 2, 4, 6, 8, 10, 12, 14, 16]\n",
    "        filter_ind = np.intersect1d(filter_ind, right_ind)\n",
    "\n",
    "    \n",
    "\n",
    "    pred_kps_ftrd = pred_kps[filter_ind]\n",
    "    ref_kps_ftrd = ref_kps[filter_ind]\n",
    "    scores_ftrd = scores[filter_ind]\n",
    "\n",
    "    if return_ind:\n",
    "        return pred_kps_ftrd, ref_kps_ftrd, scores_ftrd, filter_ind\n",
    "    else:\n",
    "        return pred_kps_ftrd, ref_kps_ftrd, scores_ftrd\n",
    "\n",
    "def do_kabsch_transform(pred_kps, ref_kps, translat_weights=[4, 3, 3, 2, 2, 1, 1, 5, 5, 3, 3, 2, 2, 1, 1]):\n",
    "    P = np.array([[kp[0], kp[1], 1] for kp in pred_kps])\n",
    "    Q = np.array([[kp[0], kp[1], 1] for kp in ref_kps])\n",
    "\n",
    "    # same pose in opposite direction (no scaling so kabsch cannot do this)\n",
    "    P_min = np.array([[-kp[0], kp[1], 1] for kp in pred_kps])\n",
    "    \n",
    "    # use reflected pose if this leads to smaller distance\n",
    "    # TODO this does not really reflect the actual value with weights and strict disctinctin between\n",
    "    # translation and rotation\n",
    "    if kabsch_rmsd(P,Q) > kabsch_rmsd(P_min, Q):\n",
    "        P = P_min\n",
    "\n",
    "    QC = centroid_weighted(Q, translat_weights)\n",
    "    Q = Q - QC\n",
    "    P = P - centroid_weighted(P, translat_weights)\n",
    "    P = kabsch_rotate(P, Q) + QC\n",
    "\n",
    "    return P\n",
    "\n",
    "def get_kabsch_distance(pred_kps, ref_kps, translat_weights=[4, 3, 3, 2, 2, 1, 1, 5, 5, 3, 3, 2, 2, 1, 1], pose_similarity_weights=[3,3,3,3,3,3,3,2,2,1,1,1,1]):\n",
    "    P = do_kabsch_transform(pred_kps, ref_kps, translat_weights=translat_weights)\n",
    "    return rmsd_weighted(P, ref_kps, weights=pose_similarity_weights)\n",
    "\n",
    "    # P = np.array([[kp[0], kp[1], 1] for kp in pred_kps])\n",
    "    # Q = np.array([[kp[0], kp[1], 1] for kp in ref_kps])\n",
    "\n",
    "    # # same pose in opposite direction (no scaling so kabsch cannot do this)\n",
    "    # P_min = np.array([[-kp[0], kp[1], 1]for kp in pred_kps])\n",
    "\n",
    "    # return min(kabsch_rmsd(P,Q), kabsch_rmsd(P_min, Q))\n",
    "\n",
    "def get_affine_tf(pred_kps, ref_kps):\n",
    "    # make sure the visibility flag is 1 always (necessary for tf)\n",
    "    ref_kps_vis = [[kp[0], kp[1], 1] for kp in ref_kps]\n",
    "\n",
    "    A, res, rank, s = np.linalg.lstsq(pred_kps, ref_kps_vis)\n",
    "    return A\n",
    "\n",
    "def warp_kp(kps, tf_mat):\n",
    "    return np.dot(kps, tf_mat)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test by warping the prediction to its own ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "test_pred_kp_ftrd, test_gt_kp_ftrd, test_pred_scores_ftrd = filter_kps(test_pred_kp, test_gt_kp, test_pred_scores, min_score=0, occluded=False, side='')\n",
    "\n",
    "# get transformation\n",
    "tf_matrix = get_affine_tf(test_pred_kp_ftrd, test_gt_kp_ftrd)\n",
    "kabsch_kp = do_kabsch_transform(test_pred_kp_ftrd, test_gt_kp_ftrd)\n",
    "\n",
    "print('kabsch: {}'.format(kabsch_kp))\n",
    "# warp pose to its own gt\n",
    "test_pred_kps_warped = warp_kp(test_pred_kp_ftrd, tf_matrix)\n",
    "print('affine: {}'.format(test_pred_kps_warped))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display prediction on og image + gt and warped on matched img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot positive prediction\n",
    "plot_image_with_kps(test_img, [test_pred_kp_ftrd], ['g'])\n",
    "plot_image_with_kps(test_img, [test_gt_kp_ftrd, test_pred_kps_warped])\n",
    "plot_image_with_kps(test_img, [test_gt_kp_ftrd, kabsch_kp])\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def calc_dist(warped_kps, ref_kps, scores, bbox):\n",
    "\n",
    "    bbox_area = (abs(bbox[2]-bbox[0]) * abs(bbox[3] -bbox[1]))\n",
    "\n",
    "    dist = 0\n",
    "    for warped_kp, ref_kp, score in zip(warped_kps, ref_kps, scores):\n",
    "        dist += sqrt((warped_kp[0]-ref_kp[0])**2 + (warped_kp[1] - ref_kp[1])**2)\n",
    "    \n",
    "    # large bboxes lead to larger distances\n",
    "    dist /= bbox_area\n",
    "\n",
    "    # more keypoints -> more distances + harder to transform -> scale superlinear\n",
    "    dist /= len(warped_kps)**1.3\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose image to find anchor for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 5\n",
    "test_img, test_target = test_dataset[test_id]\n",
    "model.eval()\n",
    "prediction = model([test_img])\n",
    "\n",
    "# get poses pred and GT\n",
    "test_pred_box, test_pred_kp, test_pred_scores = get_max_prediction(prediction)\n",
    "\n",
    "\n",
    "# plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(tensor_to_numpy_image(test_img))\n",
    "ax.scatter(np.array(test_pred_kp)[:,0],np.array(test_pred_kp)[:,1], s=10, marker='.', c='r')\n",
    "\n",
    "# plot positive prediction\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(tensor_to_numpy_image(test_img))\n",
    "#print(test_pred_scores)\n",
    "filter_inds = np.argwhere(test_pred_scores > 0).flatten()\n",
    "test_pred_kp_ftrd = test_pred_kp[filter_inds]\n",
    "ax.scatter(np.array(test_pred_kp_ftrd)[:,0],np.array(test_pred_kp_ftrd)[:,1], s=10, marker='.', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search through entire database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "dists = []\n",
    "dists_kabsch = []\n",
    "\n",
    "# try to align head, hips, schoulders and knees by translation\n",
    "translat_weights = [4, 3, 3, 2, 2, 1, 1, 5, 5, 3, 3, 2, 2, 1, 1]\n",
    "\n",
    "# pose is most defined by postition of arms \n",
    "pose_similarity_weights = [3,3,3,3,3,3,3,2,2,1,1,1,1]\n",
    "\n",
    "for idx, (test_img, test_target) in enumerate(ref_dataset):\n",
    "    print('database sample: {}'.format(idx))\n",
    "\n",
    "    test_gt_kp = test_target['keypoints'][0].detach().numpy()\n",
    "    test_gt_bbox = test_target['boxes'][0].detach().numpy()\n",
    "    # set all visible\n",
    "    test_gt_kp_all_vis = [[kp[0], kp[1], 1] for kp in test_gt_kp]\n",
    "\n",
    "    # filter\n",
    "    test_pred_kp_ftrd, test_gt_kp_ftrd, test_pred_scores_ftrd, filter_ind = filter_kps(test_pred_kp, test_gt_kp, test_pred_scores, min_score=0, occluded=False, side='', return_ind=True, filter_lr_confusion=True)\n",
    "\n",
    "    translat_weights_ftrd = np.array(translat_weights)[filter_ind]\n",
    "    pose_similarity_weights_ftrd = np.array(pose_similarity_weights)[filter_ind]\n",
    "\n",
    "    # get transformation\n",
    "    #tf_matrix = get_affine_tf(test_pred_kp_ftrd, test_gt_kp_ftrd)\n",
    "    #kabsch_kp = do_kabsch_transform(test_pred_kp_ftrd, test_gt_kp_ftrd)\n",
    "    \n",
    "    # warp pose to its own gt\n",
    "    #test_pred_kps_warped = warp_kp(test_pred_kp_ftrd, tf_matrix)\n",
    "    dist_kabsch = get_kabsch_distance(test_pred_kp_ftrd, test_gt_kp_ftrd, translat_weights=translat_weights_ftrd, pose_similarity_weights=pose_similarity_weights_ftrd)\n",
    "\n",
    "    # calculate distance\n",
    "    #dist = calc_dist(test_pred_kps_warped, test_gt_kp_ftrd, test_pred_scores_ftrd, test_gt_bbox)\n",
    "    #dist_kabsch = calc_dist(kabsch_kp, test_gt_kp_ftrd, test_pred_scores_ftrd, test_gt_bbox)\n",
    "\n",
    "    #dists += [dist]\n",
    "    dists_kabsch += [dist_kabsch]\n",
    "\n",
    "    #print('distance: {}'.format(dist)) \n",
    "    print('kabsch distance: {}'.format(dist_kabsch))\n",
    "\n",
    "    #plot_image_with_kps(test_img, [test_gt_kp_ftrd, test_pred_kps_warped])\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's plot the least distance neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get closest matches\n",
    "num_neighbors = 10\n",
    "least_ind = np.argsort(dists)[:num_neighbors]\n",
    "least_ind_kabsch = np.argsort(dists_kabsch)[:num_neighbors]\n",
    "print(least_ind_kabsch)\n",
    "\n",
    "for ind_kabsch in least_ind_kabsch:\n",
    "    test_img, test_target = ref_dataset[ind_kabsch]\n",
    "    print('database sample: {}'.format(ind_kabsch))\n",
    "\n",
    "    test_gt_kp = test_target['keypoints'][0].detach().numpy()\n",
    "    test_gt_bbox = test_target['boxes'][0].detach().numpy()\n",
    "    # set all visible\n",
    "    test_gt_kp_all_vis = [[kp[0], kp[1], 1] for kp in test_gt_kp]\n",
    "\n",
    "    # filter\n",
    "    test_pred_kp_ftrd, test_gt_kp_ftrd, test_pred_scores_ftrd, filter_ind = filter_kps(test_pred_kp, test_gt_kp, test_pred_scores, min_score=0, occluded=False, side='', return_ind=True, filter_lr_confusion=True)\n",
    "\n",
    "    translat_weights_ftrd = np.array(translat_weights)[filter_ind]\n",
    "    pose_similarity_weights_ftrd = np.array(pose_similarity_weights)[filter_ind]\n",
    "\n",
    "    # get transformation\n",
    "    kabsch_kp = do_kabsch_transform(test_pred_kp_ftrd, test_gt_kp_ftrd, translat_weights_ftrd)\n",
    "\n",
    "    # warp pose to its own gt\n",
    "    #test_pred_kps_warped = warp_kp(test_pred_kp_ftrd, tf_matrix)\n",
    "\n",
    "    # calculate distance\n",
    "    #dist = calc_dist(test_pred_kps_warped, test_gt_kp_ftrd, test_pred_scores_ftrd, test_gt_bbox)\n",
    "    #dist_kabsch = calc_dist(kabsch_kp, test_gt_kp_ftrd, test_pred_scores_ftrd, test_gt_bbox)\n",
    "\n",
    "    #print('distance: {}'.format(dist)) \n",
    "    #print('kabsch distance: {}'.format(dist_kabsch))\n",
    "\n",
    "    #plot_image_with_kps(test_img, [test_gt_kp_ftrd, test_pred_kps_warped])\n",
    "    plot_image_with_kps(test_img, [test_gt_kp_ftrd, kabsch_kp], ['w','k'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select images to represent different stages from stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 40\n",
    "stop = 60\n",
    "#anchor_poses = [50, 52, 54, 56, 59, 62, 64, 66]\n",
    "#anchor_poses_ind = [41, 44, 47, 50, 52, 53, 55, 56]\n",
    "anchor_poses_ind = [i for i in range(41,58)]\n",
    "# TODO for clustering freestyle make legs negligible and maybe increase weight of arms\n",
    "#for id in range(start, stop):\n",
    "for id in anchor_poses_ind:\n",
    "    img_tensor, target = ref_dataset[id]\n",
    "    ref_kp = target['keypoints'][0].detach().numpy()\n",
    "    plot_image_with_kps(img_tensor, [ref_kp])\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide entire database into 8 anchor poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "buckets = {}\n",
    "for anchor_id in anchor_poses_ind:\n",
    "    buckets[anchor_id] = []\n",
    "\n",
    "for id, (img_tensor, target) in tqdm(enumerate(ref_dataset)):\n",
    "    #prediction = model([img_tensor])\n",
    "    # get poses pred and GT\n",
    "    #test_pred_box, test_pred_kp, test_pred_scores = get_max_prediction(prediction)\n",
    "    ref_gt_kp = target['keypoints'][0].detach().numpy()[4:]\n",
    "\n",
    "    # no filtering necessary since groundtruths are compared\n",
    "    min_dist = 10**8\n",
    "    min_anchor = -1\n",
    "    for anchor in anchor_poses_ind:\n",
    "        _, anchor_target = ref_dataset[anchor]\n",
    "        anchor_gt_kp = anchor_target['keypoints'][0].detach().numpy()[4:]\n",
    "\n",
    "        dist = get_kabsch_distance(ref_gt_kp, anchor_gt_kp)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_anchor = anchor\n",
    "    \n",
    "    buckets[min_anchor] += [id]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in buckets.values():\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "anchor = anchor_poses_ind[idx]\n",
    "\n",
    "img_tensor, target = ref_dataset[anchor]\n",
    "anchor_kps = target['keypoints'][0].detach().numpy()[4:]\n",
    "plot_image_with_kps(img_tensor, [anchor_kps])\n",
    "\n",
    "for similar in buckets[anchor]:\n",
    "    img_tensor, target = ref_dataset[similar]\n",
    "    ref_kp = target['keypoints'][0].detach().numpy()[4:]\n",
    "\n",
    "    kabsch_kp = do_kabsch_transform(anchor_kps, ref_kp)\n",
    "    plot_image_with_kps(img_tensor, [kabsch_kp, ref_kp], ['k', 'r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a ref_pose try to match it to all anchors and print score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get ref pose\n",
    "ref_id = 100\n",
    "img_tensor, target = ref_dataset[ref_id]\n",
    "ref_kp = target['keypoints'][0].detach().numpy()[4:]\n",
    "\n",
    "plot_image_with_kps(img_tensor, [ref_kp], ['b'])\n",
    "\n",
    "for anchor in anchor_poses_ind:\n",
    "    anchor_img, anchor_target = ref_dataset[anchor]\n",
    "    anchor_gt_kp = anchor_target['keypoints'][0].detach().numpy()[4:]\n",
    "\n",
    "    dist = get_kabsch_distance(ref_kp, anchor_gt_kp)\n",
    "    print('dist to anchor {}: {}'.format(anchor, dist))\n",
    "    kabsch_kp = do_kabsch_transform(ref_kp, anchor_gt_kp)\n",
    "    plot_image_with_kps(anchor_img, [anchor_gt_kp, kabsch_kp], ['k','r'])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}