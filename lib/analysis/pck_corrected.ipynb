{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path as osp\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "# torch imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# root path of project\n",
    "from os import path as osp\n",
    "import sys\n",
    "\n",
    "# get root directory\n",
    "import re\n",
    "reg = '^.*/AquaPose'\n",
    "project_root = re.findall(reg, osp.dirname(osp.abspath(sys.argv[0])))[0]\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from lib.dataset.PoseDataset import PoseDataset\n",
    "from lib.dataset.CycleDataset import CycleDataset\n",
    "\n",
    "from lib.models.keypoint_rcnn import get_resnet50_pretrained_model\n",
    "\n",
    "# utils\n",
    "from lib.utils.slack_notifications import slack_message\n",
    "from lib.utils.select_gpu import select_best_gpu\n",
    "from lib.utils.rmsd import kabsch_rmsd, kabsch_rotate, kabsch_weighted_rmsd, centroid, centroid_weighted, rmsd, rmsd_weighted, kabsch\n",
    "\n",
    "from lib.eval.pck import pck\n",
    "\n",
    "# references import\n",
    "# source: https://github.com/pytorch/vision/tree/master/references/detection\n",
    "from references.engine import train_one_epoch, evaluate\n",
    "from references.utils import collate_fn\n",
    "\n",
    "from references.transforms import RandomHorizontalFlip\n",
    "\n",
    "from lib.matching.matching import *\n",
    "from lib.utils.visual_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\ncpu\n0, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch34-50.wth\n1, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch8-50.wth\n2, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch12-50.wth\n3, /AquaPose/AquaPose/weights/_20-03-2020-23-25_epoch9-10_min_val_loss_10000.wth\n4, /AquaPose/AquaPose/weights/_01-04-2020-17-12_epoch29-30_min_val_loss_3.673452949523926.wth\n5, /AquaPose/AquaPose/weights/_16-03-2020-14-30_epoch9-10.wth\n6, /AquaPose/AquaPose/weights/6_freestyle_ds_20-03-2020-10-00_epoch39-40.wth\n7, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch21-50.wth\n8, /AquaPose/AquaPose/weights/_16-03-2020-14-11_epoch0-10.wth\n9, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch6-50.wth\n10, /AquaPose/AquaPose/weights/_17-03-2020-18-01_epoch14-15.wth\n11, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch18-50.wth\n12, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch14-50.wth\n13, /AquaPose/AquaPose/weights/_19-03-2020-07-47_epoch99-100.wth\n14, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch26-50.wth\n15, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch41-50.wth\n16, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch25-50.wth\n17, /AquaPose/AquaPose/weights/_15-03-2020-22-39_epoch99-100.wth\n18, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch2-50.wth\n19, /AquaPose/AquaPose/weights/_20-03-2020-23-18_epoch9-10_min_val_loss_4.646083354949951.wth\n20, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch16-50.wth\n21, /AquaPose/AquaPose/weights/_01-04-2020-15-48_epoch99-100_min_val_loss_3.885883903503418.wth\n22, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch1-50.wth\n23, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch24-50.wth\n24, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch22-50.wth\n25, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch19-50.wth\n26, /AquaPose/AquaPose/weights/_15-03-2020-21-39_epoch39-40.wth\n27, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch43-50.wth\n28, /AquaPose/AquaPose/weights/_15-03-2020-21-32_epoch9-10.wth\n29, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch13-50.wth\n30, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch40-50.wth\n31, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch23-50.wth\n32, /AquaPose/AquaPose/weights/_01-04-2020-17-33_epoch49-50_min_val_loss_3.6633763313293457.wth\n33, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch30-50.wth\n34, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch49-50.wth\n35, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch17-50.wth\n36, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch47-50.wth\n37, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch39-50.wth\n38, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch11-50.wth\n39, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch32-50.wth\n40, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch5-50.wth\n41, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch37-50.wth\n42, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch9-50.wth\n43, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch20-50.wth\n44, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch31-50.wth\n45, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch36-50.wth\n46, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch3-50.wth\n47, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch29-50.wth\n48, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch38-50.wth\n49, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch45-50.wth\n50, /AquaPose/AquaPose/weights/_01-04-2020-15-01_epoch9-10_min_val_loss_3.8654736518859862.wth\n51, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch28-50.wth\n52, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch44-50.wth\n53, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch48-50.wth\n54, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch15-50.wth\n55, /AquaPose/AquaPose/weights/_20-03-2020-23-30_epoch59-60_min_val_loss_10000.wth\n56, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch42-50.wth\n57, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch46-50.wth\n58, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch35-50.wth\n59, /AquaPose/AquaPose/weights/ds_1_2_3_4_25-03-2020-16-48_epoch59-60_min_val_loss_10000.wth\n60, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch27-50.wth\n61, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch7-50.wth\n62, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch4-50.wth\n63, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch0-50.wth\n64, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch33-50.wth\n65, /AquaPose/AquaPose/weights/_16-03-2020-11-06_epoch10-50.wth\n"
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cpu = torch.device('cpu')\n",
    "print(device)\n",
    "print(cpu)\n",
    "\n",
    "weight_dir = osp.join(project_root, 'weights')\n",
    "weight_files = glob(osp.join(weight_dir,'*'))\n",
    "model = get_resnet50_pretrained_model()\n",
    "for i, f in enumerate(weight_files):\n",
    "    print('{}, {}'.format(i,f))\n",
    "model.load_state_dict(torch.load(weight_files[11], map_location=torch.device('cpu')))\n",
    "\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dirs = [osp.join(project_root,'data/vzf/freestyle/freestyle_1'), osp.join(project_root,'data/vzf/freestyle/freestyle_2'), osp.join(project_root,'data/vzf/freestyle/freestyle_3'), osp.join(project_root,'data/vzf/freestyle/freestyle_4')]\n",
    "\n",
    "test_dirs = [osp.join(project_root,'data/vzf/freestyle/freestyle_7')] \n",
    "\n",
    "train_datasets = [PoseDataset([dir], train = False, cache_predictions=True) for dir in train_dirs]\n",
    "test_pose_datasets = [PoseDataset([dir], train = False, cache_predictions=True) for dir in test_dirs]\n",
    "test_cycle_datasets = [CycleDataset([dir], cache_predictions=True, max_dist=100) for dir in test_dirs]\n",
    "\n",
    "anchor_dataset = PoseDataset([osp.join(project_root,'data/vzf/freestyle/freestyle_1')], train=False, cache_predictions=True)\n",
    "anchor_ids = [x for x in range(17,81,1)]\n",
    "anchor_dataset = torch.utils.data.Subset(anchor_dataset, anchor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pose_datasets_matching = [PoseDataset([dir], train = False, cache_predictions=True) for dir in test_dirs]\n",
    "test_pose_datasets_mle = [PoseDataset([dir], train = False, cache_predictions=True) for dir in test_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 23/23 [00:03<00:00,  7.06it/s]\n100%|██████████| 23/23 [00:03<00:00,  7.39it/s]\n100%|██████████| 23/23 [00:03<00:00,  7.36it/s]\n"
    }
   ],
   "source": [
    "for ds in test_pose_datasets:\n",
    "    ds.predict_all(model)\n",
    "for ds in test_pose_datasets_matching:\n",
    "    ds.predict_all(model)\n",
    "for ds in test_pose_datasets_mle:\n",
    "    ds.predict_all(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 974/974 [02:11<00:00,  7.40it/s]\n"
    }
   ],
   "source": [
    "for ds in test_cycle_datasets:\n",
    "    for i in tqdm(range(0,len(ds))):\n",
    "        ds.predict(model, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_index_list = [pd.get_image_name_to_index() for pd in test_pose_datasets] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmat = build_transmat(len(anchor_ids), probs=[0.25,0.30,0.20,0.15,0.05,0.05])\n",
    "flipped_transmat = build_transmat(2, [.95,.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over all corresponding datasets and do matching one per one so that image names and indexes point to the correct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n  0%|          | 0/553 [00:00<?, ?it/s]\u001b[A\n  0%|          | 1/553 [00:01<17:30,  1.90s/it]\u001b[A\n  0%|          | 2/553 [00:03<17:54,  1.95s/it]\u001b[A\n  1%|          | 3/553 [00:06<18:10,  1.98s/it]\u001b[A\n  1%|          | 4/553 [00:08<18:20,  2.00s/it]\u001b[A\n  1%|          | 5/553 [00:10<18:24,  2.02s/it]\u001b[A\n  1%|          | 6/553 [00:12<18:23,  2.02s/it]\u001b[A\n  1%|▏         | 7/553 [00:14<18:28,  2.03s/it]\u001b[A\n  1%|▏         | 8/553 [00:16<18:31,  2.04s/it]\u001b[A\n  2%|▏         | 9/553 [00:18<18:31,  2.04s/it]\u001b[A\n  2%|▏         | 10/553 [00:20<18:15,  2.02s/it]\u001b[A\n  2%|▏         | 11/553 [00:22<17:51,  1.98s/it]\u001b[A\n  2%|▏         | 12/553 [00:24<17:55,  1.99s/it]\u001b[A\n  2%|▏         | 13/553 [00:26<18:03,  2.01s/it]\u001b[A"
    }
   ],
   "source": [
    "\n",
    "for pd, pd_match, pd_mle, cd, img_name_to_index in zip(test_pose_datasets, test_pose_datasets_matching, test_pose_datasets_mle, test_cycle_datasets, img_name_to_index_list):\n",
    "\n",
    "    # get observations, obersation likelihoods...\n",
    "    obs_lik_list, observations_list, flipped_list= get_observation_likelihood(model, cd, anchor_dataset, max_stride=3, device=device)\n",
    "\n",
    "    mls_list = []\n",
    "    for obs_lik, obs, flipped_mat, seq in zip(obs_lik_list, observations_list, flipped_list, cycle_dataset.sequences):\n",
    "\n",
    "        mls = viterbi_path(np.array([1.0/len(anchor_ids)]*len(anchor_ids)), transmat, obs_lik)\n",
    "        mls_list.append(mls)\n",
    "\n",
    "        # also get most likely sequence for being flipped or not\n",
    "        flipped_observed = [flipped_mat[mls[i]][i] for i in range(0, len(mls))]\n",
    "        obslik_flipped = [[ .75 - (.5 * flipped_observed[i]) for i in range(0,len(mls))]]\n",
    "        obslik_flipped += [[ 1 - obslik_flipped[0][i] for i in range(0,len(mls))]]\n",
    "        obslik_flipped = np.array(obslik_flipped)\n",
    "      \n",
    "\n",
    "        mls_flipped = viterbi_path([.5,.5], flipped_transmat, obslik_flipped)\n",
    "\n",
    "\n",
    "\n",
    "        # for num, _ in enumerate(obs):\n",
    "        #     print('obs {}: {}'.format(num, np.array(obs_lik)[:,num]))\n",
    "\n",
    "        for obs_id, (img, match_anchor, mle_anchor) in tqdm(enumerate(zip(range(0,len(cycle_dataset)), obs, mls))):\n",
    "            _, target = cycle_dataset[img]\n",
    "            image_name = int(target['img'].split('/')[-1].split('.')[0])\n",
    "            match_warped_anchor, ax = warp_anchor_on_pred(model, cycle_dataset, img, anchor_dataset, match_anchor, True if mls_flipped[obs_id] else False)\n",
    "            mle_warped_anchor, ax = warp_anchor_on_pred(model, cycle_dataset, img, anchor_dataset, mle_anchor, True if mls_flipped[obs_id] else False)\n",
    "            if image_name in img_name_to_index.keys():\n",
    "                #print('{}: caching corrected pose'.format(image_name))\n",
    "                pd_match.prediction_cache_corrected[img_name_to_index[image_name]] = match_warped_anchor\n",
    "                pd_mle.prediction_cache_corrected[img_name_to_index[image_name]] = mle_warped_anchor\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_metric = pck(test_pose_datasets)\n",
    "pose_matching_metric = pck(test_pose_datasets_matching)\n",
    "pose_mle_metric = pck(test_pose_datasets_mle)\n",
    "\n",
    "pck_score = pose_metric.score_per_keypoint(thresholds=[x/100 for x in range(0,55,5)], corrected=False)\n",
    "inversion_error =  pose_metric.inversion_errors(thresholds=[x/100 for x in range(0,55,5)], corrected=False)\n",
    "\n",
    "pck_score_matching = pose_matching_metric.score_per_keypoint(thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "inversion_error_matching =  pose_matching_metric.inversion_errors(thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "\n",
    "pck_score_mle = pose_mle_metric.score_per_keypoint(thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "inversion_error_mle =  pose_mle_metric.inversion_errors(thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}