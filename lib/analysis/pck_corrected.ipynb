{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path as osp\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "# torch imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# root path of project\n",
    "from os import path as osp\n",
    "import sys\n",
    "\n",
    "# get root directory\n",
    "import re\n",
    "reg = '^.*/AquaPose'\n",
    "project_root = re.findall(reg, osp.dirname(osp.abspath(sys.argv[0])))[0]\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from lib.dataset.PoseDataset import PoseDataset\n",
    "from lib.dataset.CycleDataset import CycleDataset\n",
    "\n",
    "from lib.models.keypoint_rcnn import get_resnet50_pretrained_model\n",
    "\n",
    "# utils\n",
    "from lib.utils.slack_notifications import slack_message\n",
    "from lib.utils.select_gpu import select_best_gpu\n",
    "from lib.utils.rmsd import kabsch_rmsd, kabsch_rotate, kabsch_weighted_rmsd, centroid, centroid_weighted, rmsd, rmsd_weighted, kabsch\n",
    "\n",
    "from lib.eval.pck import pck\n",
    "\n",
    "# references import\n",
    "# source: https://github.com/pytorch/vision/tree/master/references/detection\n",
    "from references.engine import train_one_epoch, evaluate\n",
    "from references.utils import collate_fn\n",
    "\n",
    "from references.transforms import RandomHorizontalFlip\n",
    "\n",
    "from lib.matching.matching import *\n",
    "from lib.utils.visual_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cpu = torch.device('cpu')\n",
    "print(device)\n",
    "print(cpu)\n",
    "\n",
    "weight_dir = osp.join(project_root, 'weights')\n",
    "weight_files = glob(osp.join(weight_dir,'*'))\n",
    "model = get_resnet50_pretrained_model()\n",
    "for i, f in enumerate(weight_files):\n",
    "    print('{}, {}'.format(i,f))\n",
    "model.load_state_dict(torch.load(weight_files[0], map_location=torch.device('cpu')))\n",
    "\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dirs = [osp.join(project_root,'data/vzf/freestyle/freestyle_1'), osp.join(project_root,'data/vzf/freestyle/freestyle_2'), osp.join(project_root,'data/vzf/freestyle/freestyle_3'), osp.join(project_root,'data/vzf/freestyle/freestyle_4')]\n",
    "\n",
    "test_dirs = [osp.join(project_root,'data/vzf/freestyle/freestyle_9'), osp.join(project_root,'data/vzf/freestyle/freestyle_10'), osp.join(project_root,'data/vzf/freestyle/freestyle_11'),osp.join(project_root,'data/vzf/freestyle/freestyle_12'), osp.join(project_root,'data/vzf/freestyle/freestyle_13'), osp.join(project_root,'data/vzf/freestyle/freestyle_14')] \n",
    "\n",
    "train_datasets = [PoseDataset([dir], train = False, cache_predictions=True) for dir in train_dirs]\n",
    "test_pose_datasets = [PoseDataset([dir], train = False, cache_predictions=True) for dir in test_dirs]\n",
    "test_cycle_datasets = [CycleDataset([dir], cache_predictions=True, max_dist=100) for dir in test_dirs]\n",
    "\n",
    "anchor_dataset = PoseDataset([osp.join(project_root,'data/vzf/freestyle/freestyle_1')], train=False, cache_predictions=True)\n",
    "anchor_ids = [x for x in range(17,81,1)]\n",
    "anchor_dataset_male = torch.utils.data.Subset(anchor_dataset, anchor_ids)\n",
    "\n",
    "anchor_dataset_female = PoseDataset([osp.join(project_root,'data/vzf/freestyle/freestyle_12')], train=False, cache_predictions=True)\n",
    "\n",
    "anchor_datasets = [anchor_dataset_male, anchor_dataset_female]\n",
    "\n",
    "anchor_ids = [0, 0, 0, 0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pose_datasets_matching = [PoseDataset([dir], train = False, cache_predictions=True) for dir in test_dirs]\n",
    "test_pose_datasets_mle = [PoseDataset([dir], train = False, cache_predictions=True) for dir in test_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in test_pose_datasets:\n",
    "    ds.predict_all(model)\n",
    "for ds in test_pose_datasets_matching:\n",
    "    ds.predict_all(model)\n",
    "for ds in test_pose_datasets_mle:\n",
    "    ds.predict_all(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in test_cycle_datasets:\n",
    "    for i in tqdm(range(0,len(ds))):\n",
    "        ds.predict(model, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_index_list = [pd.get_image_name_to_index() for pd in test_pose_datasets] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flipped_transmat = build_transmat(2, [.95,.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over all corresponding datasets and do matching one per one so that image names and indexes point to the correct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_cycle_datasets[0].sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for pd, pd_match, pd_mle, cd, img_name_to_index, anchor_id in zip(test_pose_datasets, test_pose_datasets_matching, test_pose_datasets_mle, test_cycle_datasets, img_name_to_index_list, anchor_ids):\n",
    "\n",
    "    # select anchor dataset\n",
    "    anchor_dataset = anchor_datasets[anchor_id]\n",
    "\n",
    "    #transmission probability\n",
    "    transmat = build_transmat(len(anchor_dataset), probs=[0.25,0.30,0.20,0.15,0.05,0.05])\n",
    "\n",
    "    # get observations, obersation likelihoods...\n",
    "    obs_lik_list, observations_list, flipped_list= get_observation_likelihood(model, cd, anchor_dataset, max_stride=3, device=device)\n",
    "\n",
    "\n",
    "    mls_list = []\n",
    "    for obs_lik, obs, flipped_mat, seq in zip(obs_lik_list, observations_list, flipped_list, cd.sequences):\n",
    "\n",
    "        mls = viterbi_path(np.array([1.0/len(anchor_dataset)]*len(anchor_dataset)), transmat, obs_lik)\n",
    "        mls_list.append(mls)\n",
    "\n",
    "        # also get most likely sequence for being flipped or not\n",
    "        flipped_observed = [flipped_mat[mls[i]][i] for i in range(0, len(mls))]\n",
    "        obslik_flipped = [[ .75 - (.5 * flipped_observed[i]) for i in range(0,len(mls))]]\n",
    "        obslik_flipped += [[ 1 - obslik_flipped[0][i] for i in range(0,len(mls))]]\n",
    "        obslik_flipped = np.array(obslik_flipped)\n",
    "      \n",
    "\n",
    "        mls_flipped = viterbi_path([.5,.5], flipped_transmat, obslik_flipped)\n",
    "\n",
    "        #store sequence of anchor pose in cycle dataset\n",
    "        cd.obs += [obs]\n",
    "        cd.mls += [mls]\n",
    "\n",
    "        # for num, _ in enumerate(obs):\n",
    "        #     print('obs {}: {}'.format(num, np.array(obs_lik)[:,num]))\n",
    "\n",
    "        for obs_id, (img, match_anchor, mle_anchor) in tqdm(enumerate(zip(range(seq[0],seq[1]), obs, mls))):\n",
    "            _, target = cd[img]\n",
    "            image_name = int(target['img'].split('/')[-1].split('.')[0])\n",
    "            match_warped_anchor, ax = warp_anchor_on_pred(model, cd, img, anchor_dataset, match_anchor, True if mls_flipped[obs_id] else False)\n",
    "            mle_warped_anchor, ax = warp_anchor_on_pred(model, cd, img, anchor_dataset, mle_anchor, True if mls_flipped[obs_id] else False)\n",
    "            if image_name in img_name_to_index.keys():\n",
    "                #print('{}: caching corrected pose'.format(image_name))\n",
    "                pd_match.prediction_cache_corrected[img_name_to_index[image_name]] = match_warped_anchor\n",
    "                pd_mle.prediction_cache_corrected[img_name_to_index[image_name]] = mle_warped_anchor\n",
    "            #plt.show()\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_metric = pck(test_pose_datasets)\n",
    "pose_matching_metric = pck(test_pose_datasets_matching)\n",
    "pose_mle_metric = pck(test_pose_datasets_mle)\n",
    "\n",
    "pck_score = pose_metric.score_per_keypoint(model,thresholds=[x/100 for x in range(0,55,5)], corrected=False)\n",
    "inversion_error =  pose_metric.inversion_errors(model, thresholds=[x/100 for x in range(0,55,5)], corrected=False)\n",
    "\n",
    "pck_score_matching = pose_matching_metric.score_per_keypoint(model, thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "inversion_error_matching =  pose_matching_metric.inversion_errors(model, thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "\n",
    "pck_score_mle = pose_mle_metric.score_per_keypoint(model, thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "inversion_error_mle =  pose_mle_metric.inversion_errors(model, thresholds=[x/100 for x in range(0,55,5)], corrected=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds=[x/100 for x in range(0,55,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_score_merged = [0.5 * np.array([s for s in pck_score[joint]]) + 0.5 * np.array([s for s in pck_score[joint +1]]) for joint in range(1,12,2)]\n",
    "inversion_error_merged = [0.5 * np.array([s for s in inversion_error[joint]]) + 0.5 * np.array([s for s in inversion_error[joint +1]]) for joint in range(1,12,2)]\n",
    "plt.plot(thresholds, pck_score[0], label='head')\n",
    "\n",
    "joint_groups = ['shoulders', 'elbows', 'wrists', 'hips', 'knees', 'ankles']\n",
    "for scores, joint in zip(pck_score_merged, joint_groups):\n",
    "    plt.plot(thresholds, scores, label=joint)\n",
    "plt.legend()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('PCK@threshold')\n",
    "plt.show()\n",
    "\n",
    "joint_groups = ['shoulders', 'elbows', 'wrists', 'hips', 'knees', 'ankles']\n",
    "for scores, joint in zip(inversion_error_merged, joint_groups):\n",
    "    plt.plot(thresholds, scores, label=joint)\n",
    "plt.legend()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('inversion')\n",
    "plt.show()\n",
    "\n",
    "inversion_02 = [inversion_joint[4] for inversion_joint in inversion_error_merged]\n",
    "plt.bar([x for x in range(0, len(joint_groups))], inversion_02, tick_label=joint_groups, color=['#ff7f0e', 'g', 'red', '#9467bd', '#6c564b', '#e377c2'])\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.ylabel('percentage of inversion errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCK matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_score_merged = [0.5 * np.array([s for s in pck_score_matching[joint]]) + 0.5 * np.array([s for s in pck_score_matching[joint +1]]) for joint in range(1,12,2)]\n",
    "inversion_error_merged = [0.5 * np.array([s for s in inversion_error_matching[joint]]) + 0.5 * np.array([s for s in inversion_error_matching[joint +1]]) for joint in range(1,12,2)]\n",
    "plt.plot(thresholds, pck_score_matching[0], label='head')\n",
    "\n",
    "joint_groups = ['shoulders', 'elbows', 'wrists', 'hips', 'knees', 'ankles']\n",
    "for scores, joint in zip(pck_score_merged, joint_groups):\n",
    "    plt.plot(thresholds, scores, label=joint)\n",
    "plt.legend()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('PCK@threshold')\n",
    "plt.show()\n",
    "\n",
    "joint_groups = ['shoulders', 'elbows', 'wrists', 'hips', 'knees', 'ankles']\n",
    "for scores, joint in zip(inversion_error_merged, joint_groups):\n",
    "    plt.plot(thresholds, scores, label=joint)\n",
    "plt.legend()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('inversion')\n",
    "plt.show()\n",
    "\n",
    "inversion_02 = [inversion_joint[4] for inversion_joint in inversion_error_merged]\n",
    "plt.bar([x for x in range(0, len(joint_groups))], inversion_02, tick_label=joint_groups, color=['#ff7f0e', 'g', 'red', '#9467bd', '#6c564b', '#e377c2'])\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.ylabel('percentage of inversion errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCK MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_score_merged = [0.5 * np.array([s for s in pck_score_mle[joint]]) + 0.5 * np.array([s for s in pck_score_mle[joint +1]]) for joint in range(1,12,2)]\n",
    "inversion_error_merged = [0.5 * np.array([s for s in inversion_error_mle[joint]]) + 0.5 * np.array([s for s in inversion_error_mle[joint +1]]) for joint in range(1,12,2)]\n",
    "plt.plot(thresholds, pck_score_mle[0], label='head')\n",
    "\n",
    "joint_groups = ['shoulders', 'elbows', 'wrists', 'hips', 'knees', 'ankles']\n",
    "for scores, joint in zip(pck_score_merged, joint_groups):\n",
    "    plt.plot(thresholds, scores, label=joint)\n",
    "plt.legend()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('PCK@threshold')\n",
    "plt.show()\n",
    "\n",
    "joint_groups = ['shoulders', 'elbows', 'wrists', 'hips', 'knees', 'ankles']\n",
    "for scores, joint in zip(inversion_error_merged, joint_groups):\n",
    "    plt.plot(thresholds, scores, label=joint)\n",
    "plt.legend()\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('inversion')\n",
    "plt.show()\n",
    "\n",
    "inversion_02 = [inversion_joint[4] for inversion_joint in inversion_error_merged]\n",
    "plt.bar([x for x in range(0, len(joint_groups))], inversion_02, tick_label=joint_groups, color=['#ff7f0e', 'g', 'red', '#9467bd', '#6c564b', '#e377c2'])\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.ylabel('percentage of inversion errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, (img,target) in enumerate(anchor_datasets[0]):\n",
    "#     ref_kps = target['keypoints'][0].detach().numpy()\n",
    "#     ref_kps = merge_head(ref_kps)\n",
    "\n",
    "#     print('id {}'.format(id))\n",
    "#     print(target['image_id'])\n",
    "#     get_image_with_kps_skeleton(img, [ref_kps])\n",
    "#     plt.gca().invert_xaxis()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for ds, ds_match, anchor_id in zip(test_pose_datasets_mle, test_pose_datasets_matching, anchor_ids):\n",
    "#     anchor_dataset = anchor_datasets[anchor_id]\n",
    "#     for idx, (img, target) in enumerate(ds):\n",
    "#         # if ds.prediction_cache_corrected[idx] is None:\n",
    "#         #     count += 1\n",
    "#         #     continue\n",
    "#         _, pred, scores = ds.predict(model,idx, corrected=False)\n",
    "#         ref_kps = merge_head(target['keypoints'][0].detach().numpy())\n",
    "        \n",
    "#         match_inds, match_scores, match_flipped = get_most_similar_ind_and_scores(pred, scores, anchor_dataset, num=5, translat_weights=T_WEIGHTS, kp_weights=KP_WEIGHTS)\n",
    "\n",
    "#         pred = merge_head(pred)\n",
    "#         scores = merge_head(scores)\n",
    "#         # warped = ds.predict(model, idx, corrected=True)\n",
    "#         # matched = ds_match.predict(model, idx, corrected=True)\n",
    "#         filter_ind = np.array([x for x in range(0,13)])[scores > 0]\n",
    "#         filter_ind = np.append(filter_ind, np.array([9,10,11,12]))\n",
    "\n",
    "#         print('ID {}'.format(idx))\n",
    "#         print('original')\n",
    "#         plt.imshow(tensor_to_numpy_image(img))\n",
    "#         plt.axis('off')\n",
    "#         plt.grid(b=None)\n",
    "#         plt.gca().invert_xaxis()\n",
    "#         plt.show()\n",
    "\n",
    "#         print('prediction:')\n",
    "#         get_image_with_kps_skeleton(img, [pred], filter_ind=filter_ind, color_list=['r'])\n",
    "#         plt.gca().invert_xaxis()\n",
    "#         plt.show()\n",
    "        \n",
    "#         # print('mle result: ')\n",
    "#         # get_image_with_kps_skeleton(img, [warped])\n",
    "#         # plt.gca().invert_xaxis()\n",
    "#         # plt.show()\n",
    "#         print('gt pose: ')\n",
    "#         get_image_with_kps_skeleton(img, [ref_kps], color_list=['lime'])\n",
    "#         plt.gca().invert_xaxis()\n",
    "#         plt.show()\n",
    "\n",
    "#         print('matched')\n",
    "#         for match_ind, match_flip in zip(match_inds, match_flipped):\n",
    "#             print('match_id {}'.format(match_ind))\n",
    "#             match_warped_anchor, ax = warp_anchor_on_pred(model, ds, idx, anchor_dataset, match_ind, True if match_flip else False)\n",
    "            \n",
    "#             get_image_with_kps_skeleton(img, [match_warped_anchor])\n",
    "#             plt.gca().invert_xaxis()\n",
    "#             plt.show()\n",
    "        \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stroke Rate extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# transform into binary list\n",
    "def sawtooth_crash(seq_list, crash_height):\n",
    "    crashes = []\n",
    "    for seq in seq_list:\n",
    "        #Search crash to start sequence or if current anchor pose index is low enough assume that crash just happened\n",
    "        if seq[0] < 30:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = len(seq) - 1\n",
    "            for i in range(0, len(seq) - 1):\n",
    "                if seq[i] - seq[i+1] > crash_height:\n",
    "                    start = i\n",
    "                    break\n",
    "        seq_crashes = [1]\n",
    "        for i in range(start + 1, len(seq) - 1):\n",
    "            if seq[i] - seq[i+1] > crash_height:\n",
    "                seq_crashes.append(1)\n",
    "            else:\n",
    "                seq_crashes.append(0)\n",
    "        crashes.append(seq_crashes)\n",
    "    return crashes\n",
    "\n",
    "def get_stroke_durations(seq_list, crash_height, fps_list):\n",
    "        crashes = sawtooth_crash(seq_list, crash_height)\n",
    "        # return per sequence a list with half stroke (1 phase) durations\n",
    "        stroke_durations = []\n",
    "        for seq, fps in zip(crashes, fps_list):\n",
    "            seq_phase_durations = []\n",
    "            num_frames = 1\n",
    "            for item in seq[1:]:\n",
    "                if item == 1:\n",
    "                    seq_phase_durations.append(num_frames / fps)\n",
    "                    num_frames = 1\n",
    "                else: \n",
    "                    num_frames += 1\n",
    "            #count last stroke if approaching sawtooth\n",
    "            if num_frames > crash_height:\n",
    "                seq_phase_durations.append(num_frames / fps)\n",
    "            stroke_durations.append(seq_phase_durations)\n",
    "        \n",
    "        return stroke_durations\n",
    "\n",
    "def get_mean_stroke_rate(seq_list, crash_height, fps_list):\n",
    "    stroke_durations = get_stroke_durations(seq_list, crash_height, fps_list)\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    stroke_durations = flatten(stroke_durations)\n",
    "    return 60.0 / mean(stroke_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps_set contains fps for ech sequence\n",
    "fps_set = [25,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_stroke_rate_predicted = get_mean_stroke_rate(mls_list, 30, fps_set)\n",
    "stroke_durations = get_stroke_durations(mls_list, 30, fps_set)\n",
    "print('phase durations: {}'.format(stroke_durations))\n",
    "print('mean stroke rate: {}'.format(mean_stroke_rate_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_durations = test_cycle_datasets[0].get_phase_durations(fps_set)\n",
    "mean_stroke_rate = test_cycle_datasets[0].get_mean_stroke_rate(fps_set)\n",
    "print('phase durations: {}'.format(phase_durations))\n",
    "print('mean stroke rate: {}'.format(mean_stroke_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_stroke_rate(self, fps_list):\n",
    "    phase_durations = get_phase_durations(self,fps_list)\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    phase_durations = flatten(phase_durations)\n",
    "\n",
    "    mean_stroke_duration = mean(phase_durations) * 2\n",
    "\n",
    "    return 60.0 / mean_stroke_duration\n",
    "\n",
    "# in seconds\n",
    "def get_phase_durations(self, fps_list):\n",
    "    # return per sequence a list with half stroke (1 phase) durations\n",
    "    phase_durations = []\n",
    "    for seq, fps in zip(self.sequences, fps_list):\n",
    "        seq_phase_durations = []\n",
    "        num_frames = 1\n",
    "        for item_id in range(seq[0]  + 1, seq[1]):\n",
    "            if self.items[item_id]['offset'] == 0:\n",
    "                seq_phase_durations.append(num_frames / fps)\n",
    "                num_frames = 1\n",
    "            else:\n",
    "                num_frames += 1\n",
    "        phase_durations.append(seq_phase_durations)\n",
    "        \n",
    "    return phase_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_offset = [[item_dict['offset']==0 for _, item_dict in torch.utils.data.Subset(test_cycle_datasets[-1], [i for i in range(seq[0], seq[1])]) if item_dict['phase'] != 'turn'] for seq in test_cycle_datasets[-1].sequences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_offset = [[1 if b else 0 for b in p] for p in phase_offset]\n",
    "\n",
    "\n",
    "# plt.plot([x for x in range(0, len(mls))][1:201], mls[1:201])\n",
    "# #plt.plot([x for x in range(0, len(mls))], obs)\n",
    "# odd = 1\n",
    "# for frame,stroke in enumerate(phase_offset[0]):\n",
    "#     if frame > 201:\n",
    "#         break\n",
    "#     if stroke==1:\n",
    "#         if odd == 1:\n",
    "#             plt.axvline(x=frame, color='r')\n",
    "#         odd = 1 - odd\n",
    "\n",
    "# plt.xlabel('frame')\n",
    "# plt.ylabel('anchor id')\n",
    "\n",
    "for cd in test_cycle_datasets:\n",
    "    phase_offset = [[item_dict['offset']==0 for _, item_dict in torch.utils.data.Subset(cd, [i for i in range(seq[0], seq[1])]) if item_dict['phase'] != 'turn'] for seq in cd.sequences]\n",
    "    phase_offset = [[1 if b else 0 for b in p] for p in phase_offset]\n",
    "\n",
    "    for seq_id, seq in enumerate(cd.sequences):\n",
    "        x_range = [x for x in range(seq[0], seq[1])]\n",
    "        plt.plot(x_range, cd.mls[seq_id])\n",
    "\n",
    "        odd = 1\n",
    "        for frame,stroke in enumerate(phase_offset[seq_id]):\n",
    "            if stroke==1:\n",
    "                if odd == 1:\n",
    "                    plt.axvline(x=x_range[frame], color='r')\n",
    "                odd = 1 - odd\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get average error of stroke durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cd in test_cycle_datasets:\n",
    "    print('{},{}'.format(cd.sequences, cd.mls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many phases are taken together\n",
    "intervals = [1, 2, 3, 4 ,5, 6]\n",
    "errors = [[] for x in intervals]\n",
    "\n",
    "fps_set = [50,50,50, 25 ,50 ,25]\n",
    "for cycle_ds, fps, anchor_id in zip(test_cycle_datasets, fps_set, anchor_ids):\n",
    "    anchor_dataset = anchor_datasets[anchor_id]\n",
    "\n",
    "    # one fps entry per sequence\n",
    "    fps_list = [fps for x in range(0,len(cycle_ds.sequences))]\n",
    "\n",
    "    # gt phases durations\n",
    "    # default per half stroke cycle so produce sum of two entries\n",
    "    gt_durations = cycle_ds.get_phase_durations(fps_list)\n",
    "    gt_durations_merged = []\n",
    "    \n",
    "    # for each sequence\n",
    "    for gt_dur_seq in gt_durations:\n",
    "        gt_durations_merged_seq = []\n",
    "        for i in range(0, len(gt_dur_seq) - 1, 2):\n",
    "            gt_durations_merged_seq += [gt_dur_seq[i] + gt_dur_seq[i + 1]]\n",
    "        gt_durations_merged += [gt_durations_merged_seq]\n",
    "    gt_durations = gt_durations_merged\n",
    "\n",
    "    # dt phases durations\n",
    "    dt_durations = get_stroke_durations(cycle_ds.mls, len(anchor_dataset)/2, fps_list)\n",
    "    print(cycle_ds)\n",
    "    print(gt_durations)\n",
    "    print(dt_durations)\n",
    "    \n",
    "    for interval_id, interval in enumerate(intervals):\n",
    "        for gt_seq, dt_seq in zip(gt_durations, dt_durations):\n",
    "            #print(gt_seq)\n",
    "            #print(dt_seq)\n",
    "            #merge over interval\n",
    "            gt_seq_interval = []\n",
    "            dt_seq_interval = []\n",
    "            for i in range(0, min(len(gt_seq),len(dt_seq)) - (interval - 1), interval):\n",
    "                gt_seq_interval += [sum([gt_seq[i+j] for j in range(0, interval)])]\n",
    "                dt_seq_interval += [sum([dt_seq[i+j] for j in range(0,interval)])]\n",
    "\n",
    "            # get absolute difference\n",
    "            abs_diff = np.array(gt_seq_interval) - np.array(dt_seq_interval)\n",
    "            abs_diff = list(np.absolute(abs_diff))\n",
    "\n",
    "            # append LIST to error list of interval\n",
    "            errors[interval_id] += abs_diff\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = []\n",
    "for interval_error, interval in zip(errors, intervals):\n",
    "    interval_error = np.array(interval_error)\n",
    "    mean_val = np.mean(interval_error)/interval\n",
    "\n",
    "    averages += [mean_val]\n",
    "\n",
    "plt.plot(intervals, averages)\n",
    "plt.xticks([x for x in intervals])\n",
    "plt.xlabel('strokes per interval')\n",
    "plt.ylabel('average stroke duration error (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}